{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFM_Reddit_2019.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "18goEU8QCir1sAAVNPXyvgVth9n1Xl0Og",
      "authorship_tag": "ABX9TyP6ZW91yFpY8KiRbGknntNa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CAVA1611/neural-networks/blob/main/TFM_Reddit_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM9zRBBl_i8a"
      },
      "source": [
        "# Predicción con Series Temporales de la carga en un centro de datos\n",
        "\n",
        "**Autor:** Cesar Velasco Arias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1QRGcM__tym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ab0c01-ca6d-4ab0-c52d-7c13015d02ff"
      },
      "source": [
        "# Importacion de librerias\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from time import sleep\n",
        "import re \n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "np.random.seed(80)\n",
        "random.seed(80)\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUktnJDKD8ih"
      },
      "source": [
        "path = '/content/drive/MyDrive/TFM/csv_2019'\n",
        "year = '2019'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OjQ6S1qBiH6"
      },
      "source": [
        "\n",
        "def files_verification():\n",
        "  meses = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
        "  list_dir=[]\n",
        "  files_to_read=[]\n",
        "  files_not_found=[] \n",
        "  count = 0 \n",
        "  try:\n",
        "    list_dir = os.listdir(path)\n",
        "    if os.path.isdir(path) and len(list_dir)>0:\n",
        "      for mes in meses:\n",
        "        file_to_read = 'LoadReddit-60minutes-'+ str(year)+'-'+ str(mes)+'-PLN.csv'\n",
        "        if os.path.isfile(os.path.join(path, file_to_read)):\n",
        "            count+=1\n",
        "            files_to_read.append(file_to_read)\n",
        "        else:\n",
        "          files_not_found.append(file_to_read)\n",
        "  except:  \n",
        "    print('No se ha encontrado el directorio')\n",
        "  \n",
        "  print('Se han encontrado: {} correspondientes al año {}'.format(count,year))\n",
        "  for f in files_to_read:\n",
        "    print('-',f)\n",
        "  print('\\nHace falta {} archivos'.format(12-count))\n",
        "  for nf in files_not_found:\n",
        "    print('-',nf)\n",
        "  print('\\n')\n",
        "  return files_to_read"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jTDpPPuZe0X"
      },
      "source": [
        "\n",
        "def data_reading(path, year):\n",
        "  global df\n",
        "  meses = {'enero': '01', 'febrero':'02', 'marzo':'03', 'abril':'04', 'mayo':'05', \n",
        "           'junio':'06', 'julio':'07', 'agosto':'08','septiembre':'09',\n",
        "           'octubre':'10', 'noviembre':'11', 'diciembre':'12'}\n",
        "  month_conc = []\n",
        "  files_to_read = files_verification()    \n",
        "  for mes_k, mes_v in zip(meses.keys(), meses.values()):\n",
        "    for file in files_to_read:\n",
        "      pa = file.split('-')\n",
        "      for p in pa:\n",
        "        if p == mes_v:\n",
        "          mes_k = pd.read_csv(os.path.join(path, file), \n",
        "                              header=None, names=['Date', 'number_of_post'], \n",
        "                              parse_dates=['Date']) \n",
        "          month_conc.append(mes_k)\n",
        "          #print('Dimension para el mes {}: {}'.format(mes_k,mes_k.shape))\n",
        "  df = pd.concat(month_conc, axis=0)\n",
        "  print('*'*30)\n",
        "  print('\\nDimension de df -> ',df.shape)\n",
        "\n",
        "  return (df, meses)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2x3fvfXnzkD"
      },
      "source": [
        "def graphic_1(start_month,end_month,year):\n",
        "  global df_time, train_scaler, test_scaler\n",
        "  \n",
        "  df_time, meses = data_reading(path, year)\n",
        "  df_time = df_time.set_index('Date')\n",
        "  for mes_k, mes_v in zip(meses.keys(), meses.values()):\n",
        "    if start_month == mes_k:\n",
        "      start_month_index = mes_v\n",
        "  for mes_k, mes_v in zip(meses.keys(), meses.values()):\n",
        "    if end_month == mes_k:\n",
        "      end_month_index = mes_v\n",
        "  if start_month_index > end_month_index:\n",
        "    print(\"Mal ingreso de mes, el mes de inicio no puede ser despues del segundo\")\n",
        "  else:\n",
        "    print(\"Datos para graficar desde {} hasta {}\".format(start_month, end_month))\n",
        "    steps = int(len(df_time)*0.2)\n",
        "    train = df_time[:-steps]\n",
        "    test  = df_time[-steps:]\n",
        "    max = train.max()\n",
        "    min = train.min()\n",
        "    train_scaler=  train['number_of_post'].apply(lambda x: (x-min)/(max-min))\n",
        "    test_scaler= test['number_of_post'].apply(lambda x: (x-min)/(max-min)) \n",
        "    fig, (ax1, ax2)=plt.subplots(2, figsize=(20, 8))\n",
        "    df_time.plot(ax=ax1, grid=True)\n",
        "    train_scaler.plot(ax=ax2, grid=True)\n",
        "    test_scaler.plot(ax=ax2, grid=True)\n",
        "    ax1.legend(['Datos'])\n",
        "    ax2.legend(['Train', 'Test'])\n",
        "    ax1.set_title('Datos Originales')\n",
        "    ax2.set_title('Datos Normalizados')\n",
        "    plt.subplots_adjust(hspace=0.8)\n",
        "  \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkaALxqVZEir"
      },
      "source": [
        "def data_for_rn(df_train, df_test, n_obs):\n",
        "  #n_obs -> es el numero de observacione que se toman para predecir un valor\n",
        "  # Con los datos de train se debe dividir en un array \"X_train\" y  \"y_train\"; este es un formato necesario para la prediccion en series\n",
        "  # temporales. En \"X_train\" se encontaran el numero de observaciones que leera el modelo para predecir y en \"y_train\" se encontrara\n",
        "  # el valor a predecir. \n",
        "  # Ambos tendran la misma dimension en cuanto a numero de filas.\n",
        "  # Para el caso de este trabajo se ha escogido como numero de observaciones 20, es decir que se leeran 20 datos para predecir 1\n",
        "\n",
        "  Obs = n_obs\n",
        "  X_train = np.atleast_3d(np.array([df_train[start:start + Obs] for start in range(0, df_train.shape[0] - Obs)]))\n",
        "  y_train = df_train[Obs:]\n",
        "\n",
        "  # Para probar el modelo entrenado con los datos de test se tiene que colocar en el mismo formato.\n",
        "  X_test = np.atleast_3d(np.array([df_test[start:start + Obs] for start in range(0, df_test.shape[0] - Obs)]))\n",
        "  y_test = df_test[Obs:]\n",
        "  X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "  \n",
        "  return (X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBliYhyXadlx"
      },
      "source": [
        "## Lectura y Grafica de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXq4aStbag4x"
      },
      "source": [
        "start_month = 'enero'\n",
        "end_month = 'diciembre'\n",
        "graphic_1(start_month,end_month,year)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq-rm5PqcFzW",
        "outputId": "2e7c243c-92f6-4663-9f31-8c1dcedf6e69"
      },
      "source": [
        "# Obtener \"X\" y \"y\" train y test para las redes neuronales (solo es necesario realizar una vez)\n",
        "# se trabaja con los datos escalados\n",
        "(X_train, y_train, X_test, y_test) = data_for_rn(train_scaler, test_scaler, 20)\n",
        "print(\"Dimension de X_train: \", X_train.shape)\n",
        "print(\"\\nDimension de y_train: \", y_train.shape)\n",
        "print(\"\\nDimension de X_test: \", X_test.shape)\n",
        "print(\"\\nDimension de y_test: \", y_test.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimension de X_train:  (5813, 20, 1)\n",
            "\n",
            "Dimension de y_train:  (5813, 1)\n",
            "\n",
            "Dimension de X_test:  (1438, 20, 1)\n",
            "\n",
            "Dimension de y_test:  (1438, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}